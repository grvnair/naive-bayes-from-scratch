{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b7fb71",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Naive Bayes Algorithm (GaussianNB)}}$<br>\n",
    "***\n",
    "\n",
    "Naive Bayes is a supervised classification algorithm based on Bayes' theorem. In layman's term, it is a probabilistic classifier, which means it predicts on the basis of the probability of an event.\n",
    "\n",
    "<li> It can be used for binary as well as multi-calss classifications.</li>\n",
    "<li> It performs well in multi-class classifications as compared to other algorithms.</li>\n",
    "<li> It is widely used for spam detection and sentiment analysis.</li>\n",
    "    \n",
    "\n",
    "There are three common types of Naive Bayes classifier are:\n",
    "\n",
    "<li> Gaussian: It is used in classification and it assumes that features follow a normal distribution.</li>\n",
    "<li> Multinomial: MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice).</li>\n",
    "<li> Bernoulli: BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. </li>\n",
    "\n",
    "It makes a naive assumption that every pair of features being classified are independent of each other. Hence it is called Naive Bayes.\n",
    "\n",
    "Mathematically, we can write it as:\n",
    "$$\n",
    "P(X|y) = \\frac{P(y|X) .P(X)}{P(y)}\n",
    "$$\n",
    "\n",
    "<li>P(X|y) = class conditional probability(probability of X when event y has already happened),</li>\n",
    "<li>P(y|X) = Posterior probability(probability of y when event X has already happened),</li>\n",
    "<li>P(X) = prior probability of X(probability of event X happening),</li>\n",
    "<li>P(y) = prior probability of y(probability of event y happening)</li>\n",
    "\n",
    "We can write our feature vectors X as:\n",
    "\n",
    "\\begin{align}\n",
    "X = (x_{1}, x_{2}, x_{3}, .... ,x_{n})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Assuming that all the features are mutually independent:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|X) = \\frac{P(x_{1}|y) .P(x_{2}|y) .P(x_{3}|y) ....P(x_{n}|y) .P(y)}{P(X)}\n",
    "\\end{align}\n",
    "\n",
    "Now we will be selecting the class with highest probability:\n",
    "\\begin{align}\n",
    "y = argmax_{y} P(y|X)\n",
    "\\end{align}\n",
    "\n",
    "Putting in P(y|X), as all the features are mutually independent:\n",
    "\\begin{align}\n",
    "y = argmax_{y} \\frac{P(x_{1}|y) .P(x_{2}|y) .P(x_{3}|y) ....P(x_{n}|y) .P(y)}{P(X)}\n",
    "\\end{align}\n",
    "\n",
    "As we are only considered with the probability of y, we can omit P(X)\n",
    "\n",
    "\\begin{align}\n",
    "y = argmax_{y} P(x_{1}|y) .P(x_{2}|y) .P(x_{3}|y) ....P(x_{n}|y) .P(y)\n",
    "\\end{align}\n",
    "\n",
    "Multiplying all the probabilities will result in a very small number and hence to avoid this, we will apply the log function:\n",
    "\n",
    "\\begin{align}\n",
    "y = argmax_{y} log(P(x_{1}|y)) +log(P(x_{2}|y)) +log(P(x_{3}|y)) +....+ log(P(x_{n}|y)) .P(y)\n",
    "\\end{align}\n",
    "\n",
    "For Gaussian distribution, class conditional probability will be given as:\n",
    "\n",
    "\\begin{align}\n",
    "P(x_{i}|y) = \\frac{1}{\\sqrt{2πσ^{2}_{y}}}  .exp (-\\frac{(x_{i} - µ_{y})^{2}}{2σ^{2}_{y}})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26f9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f6802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape #X is a numpy n-dimension array\n",
    "        self._classes = np.unique(y) #To find unique elements of an array\n",
    "        n_classes = len(self._classes) #To specify number of classes\n",
    "        \n",
    "        # initializing mean, variance and priors with zeros\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype = np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype = np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype = np.float64)\n",
    "        \n",
    "        for index, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[c,:] = X_c.mean(axis=0)\n",
    "            self._var[c,:] = X_c.var(axis=0)\n",
    "            self._priors[c] = X_c.shape[0] / float(n_samples)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "        \n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "        \n",
    "        # Calculating posterior probability for each class\n",
    "        for index, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[index])\n",
    "            posterior = np.sum(np.log(self._pdf(index, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        # return class with highest probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "            \n",
    "    def _pdf(self, class_index, x):\n",
    "        mean = self._mean[class_index]\n",
    "        var = self._var[class_index]\n",
    "        numerator = np.exp(-((x - mean)**2)/(2*var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        \n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5e04f",
   "metadata": {},
   "source": [
    "### Implementing the algorithm to IRIS dataset from sklearn and checking the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cb165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Gaussian Naive Bayes model is:  0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "    \n",
    "    df = load_iris()\n",
    "    \n",
    "    X, y = df.data, df.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=1234)\n",
    "    \n",
    "    nb = NaiveBayes()\n",
    "    \n",
    "    nb.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = nb.predict(X_test)\n",
    "    \n",
    "    print(\"The accuracy of the Gaussian Naive Bayes model is: \", accuracy(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
